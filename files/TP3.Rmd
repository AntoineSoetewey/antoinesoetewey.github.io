---
title: 'LSTAT2110 - Analyse des données : TP3'
author: "[Antoine Soetewey](mailto:antoine.soetewey@uclouvain.be), [Johan Segers](mailto:johan.segers@uclouvain.be) et Baptiste Feraud"
date: "2018-2019"
output:
  html_document:
    number_sections: yes
    toc: true
# output:
#     pdf_document:
#       toc: true
#       toc_depth: 3
---

```{r setup, include=FALSE}
#knitr::opts_knit$set(root.dir = "/Users/antoine/Google Drive/PhD/LSTAT2110/TP/TP3")
knitr::opts_chunk$set(comment = "")
```

<!-- The code hereafter forces all figures to appear exactly where they were placed in the text  -->
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

# Déroulement des TPs

* TP1 :
    + Introduction à R
    + Statistiques descriptives
    + Manipulation de matrices
    + Analyse en Composantes Principales (ACP)
* TP2 :
    + Classification (*clustering* en anglais)
    + Analyse discriminante
* TP3 (pas LSTAT2110A) :
    + Analyse des correspondances simples
    + Discrétiser une variable continue
    + Analyse des correspondances multiples
    
# Relations entre variables

![](effobs.png)

Quelles relations entre ces 2 variables ? Il existe plusieurs outils statistiques :

* Test d'indépendance (*chi-carré*)
* Force de l'association ($\phi$, *V* de Cramer)
* Analyse factorielle des correspondances

## Test d'indépendance

Le test d'indépendance entre deux variables revient à mesurer l’écart entre ce qu’on observe et ce que l’on s’attend à observer dans une situation théorique d’indépendance.

![](effobs_att.png)

Les hypothèses sont les suivantes : $$H_0 : P(i,j) = P(i) \times P(j) \text{ (les variables sont indépendantes)}$$  $$H_1 : P(i,j) \neq P(i) \times P(j) \text{ (les variables sont dépendantes)}$$

En R :

```{r}
# Create the data frame
data <- data.frame(
   Non_fumeur = c(23,10,3), 
   Fumeur = c(19,23,22),
   row.names = c("Aucun verre", "De 1 à 4 verres", "Plus de 4 verres"),
   stringsAsFactors = FALSE
)
# Print the data frame	
print(data)

# Chi-squared test
test <- chisq.test(data, correct = FALSE)
test

# Affichage des effectifs espérés :
round(test$expected, 2)
# Affichage des résidus :
round(test$residuals, 2)
```

Si l’hypothèse d’indépendance est rejetée (par exemple : *p*-valeur < 0.05) comme c'est le cas dans notre exemple, les variables ne sont pas indépendantes et il est alors intéressant d’observer la contribution de chacune des modalités par rapport à ce rejet.

$\rightarrow$ Analyse factorielle des correspondances simples (AFCS)

# AFCS sur une base de données

On peut appliquer l’analyse des correspondances simples directement sur une base de donnée (à la place d’un tableau croisé) si :

* Lorsque l’on additionne toutes les valeurs d’une ligne, on peut tirer du résultat une interprétation pertinente
(également avec les autres opérations mathématiques telle la moyenne)
* Lorsque l’on additionne toutes les valeurs d’une colonne, on peut tirer du résultat une interprétation pertinente
(également avec les autres opérations mathématiques telle la moyenne)

## Eurojobs

```{r}
Eurojobs <- read.csv(file="Eurojobs.csv", sep=",", dec=".", header=TRUE) # importation des données
rownames(Eurojobs) <- Eurojobs$Country #définition des noms des pays comme identifiants des lignes
Eurojobs$Country <- NULL #retrait de la variable pays

head(Eurojobs)
```

```{r}
round(rowSums(Eurojobs), digits = 2)
round(colMeans(Eurojobs), digits = 2)
```

## `CA()`

```{r}
library("FactoMineR")
EurojobsCA <- CA(Eurojobs, ncp = 5, graph = FALSE)
EurojobsCA
plot(EurojobsCA)
```

### Valeurs propres

```{r}
round(EurojobsCA$eig, digits = 2)
```

### Sorties clés pour les colonnes

```{r}
lapply(EurojobsCA$col, round, 2)
```

### Sorties clés pour les lignes

```{r}
lapply(EurojobsCA$row, round, 2)
```

### Représentation - regarder `$cos2`

Quel est la modalité et l'individu avec la meilleure (pire) représentation dans le premier plan factoriel (dimension 1 et 2) ?

```{r}
round(sort(rowSums(EurojobsCA$col$cos2[,1:2])), digits = 3)
round(sort(rowSums(EurojobsCA$row$cos2[,1:2])), digits = 3)
```

`Agr` (`PS`) est la colonne la mieux (moins bien) représentée sur le premier plan factoriel. `Belgium` (`Spain`) est la ligne la mieux (moins bien) représentée sur le premier plan factoriel.

### Interprétation des axes - regarder `$contrib`

Pour interpréter un certain axe, il faut regarder les contributions (en %) dans chacune des composantes principales :

```{r}
round(EurojobsCA$col$contrib[,c(1,2)], digits = 3)
```

Les colonnes qui contribuent le plus à Dim.1 et à Dim.2 sont les plus importantes pour expliquer la variabilité de l'ensemble de données.
Les colonnes qui ne contribuent pas beaucoup à une dimension ou qui contribuent aux dernières dimensions sont moins importantes.

# AFCS vs ACP

En travaillant avec l’AFCS :

* On change de standardisation
(de centré-réduit à une division par la racine du profil moyen)
* On change de métrique (d’Euclidienne à Chi-carré)
* **On obtient une représentation simultanée
des variables et des individus**

## Comparaison des valeurs propres entre ACP et AFCS

```{r}
par(mfrow=c(1,2))

EurojobsPCA <- PCA(Eurojobs, graph=FALSE)
barplot(EurojobsPCA$eig[,"percentage of variance"], ylab = "Percentage of variance", main = "En ACP")
barplot(EurojobsCA$eig[,"percentage of variance"], ylab = "Percentage of variance", main = "En AFCS")
```

# Discrétiser une variable continue

Il existe plusieurs façons de discrétiser une variable continue :

* Classes de taille égale (fonction de l’étendue)
* Classes basées sur les quantiles (effectifs égaux)
* Classes 'naturelles' (algorithme k-means - cf. Clustering)
* Avec la fonction `cut()` :
    + Classes de taille égale (fonction de l’étendue)
    + Classes prédéfinies (ex : classes d’âge)
  
$\rightarrow$ Possibilité d’effectuer ensuite des statistiques descriptives sur ces nouvelles classes !

En R :

```{r}
load("Ronfle.rda") # détails de cette base de données plus bas

Ronfle$AgeDiscr <- cut(Ronfle$Age, breaks = c(min(Ronfle$Age), 40, 50, 60, 70, max(Ronfle$Age)), labels=c("<40", "40-49", "50-59", "60-69", ">=70"), include.lowest = TRUE, right = FALSE)

summary(Ronfle$AgeDiscr)
```

# Analyse des Correspondances Multiples (ACM)

## Objectif de l’Analyse des Correspondances Multiples

En ACM, l’objectif est d’obtenir une représentation graphique où ...

* toutes les modalités des variables initiales et les individus sont représentés sur un même plan
* la proximité (l’éloignement) d’une modalité et d’un individu prend un sens intrinsèque
* le centre du plan ou de l’axe (l’origine) a également du sens intrinsèque

<!-- ## La table disjonctive complète -->

<!-- Lorsque l’on applique une ACM, le logiciel transforme la base de données initiale contenant *p* variables discrètes (ou discrétisées) en une table disjonctive complète (*Z*) contenant *Q* variables binaires en colonnes (avec $Q = \sum^p_{j=1} (I_j - 1)$ où $I_j$ est le nombre de niveau de la variable $j$). -->

<!-- ![](tabdisj.png) -->
<!-- ## La table de Burt -->

## La base de données `Ronfle.rda`

* Données concernant 100 individus
* A propos desquels les variables suivantes sont disponibles : Âge (+ version discrétisée), Poids, Taille, IMC (+ version discrétisée), Alcool (+ version discrétisée), Sexe, Ronfle, Tabac
* Format de données .rda (rdata) : pour l’importation, passer par `load("Ronfle.rda")`

### ACM en R - fonction `MCA()`

```{r}
Ronfle.MCA <- Ronfle[, c("Sexe", "Ronfle", "Tabac", "AlcoolDisc", "AgeDisc", "IMCDisc")]
res.MCA <- MCA(Ronfle.MCA, ncp=5, graph = FALSE)
res.MCA
```

### Les valeurs propres

```{r}
round(res.MCA$eig, digits = 3)
barplot(res.MCA$eig[,"percentage of variance"], ylab = "Percentage of variance")
```

### Les résultats pour les variables

```{r}
lapply(res.MCA$var, round, 3)
```

### Résultats graphiques

```{r}
plot(res.MCA, cex = 0.7)
```

### Représentation - regarder `$cos2`

Quel est la modalité et l'individu avec la meilleure (pire) représentation dans le premier plan factoriel (dimension 1 et 2) ?

```{r}
round(sort(rowSums(res.MCA$var$cos2[,1:2])), digits = 3)
round(sort(rowSums(res.MCA$ind$cos2[,1:2])), digits = 3)
```

`Aucun verre` (`Surpoids`) est la variable la mieux (moins bien) représentée sur le premier plan factoriel. L'individu `83` (`13`) est l'individu le mieux (moins bien) représenté sur le premier plan factoriel.

### Interprétation des axes - regarder `$contrib`

Pour interpréter un certain axe, il faut regarder les contributions (en %) dans chacune des composantes principales :

```{r}
round(res.MCA$var$contrib[,c(1,2)], digits = 3)
```

Les variables qui contribuent le plus à Dim.1 et à Dim.2 sont les plus importantes pour expliquer la variabilité de l'ensemble de données.
Les variables qui ne contribuent pas beaucoup à une dimension ou qui contribuent aux dernières dimensions sont moins importantes.

<!-- ![](plot.MCA.png) -->

<!-- # L’analyse factorielle multiple (AFM) -->

<!-- * Plusieurs groupes de variables continues ou discrètes (on impose une cohérence interne des groupes) -->
<!-- * L’objectif est de trouver les axes factoriels principaux de l’ensemble des variables ainsi que ceux de chaque groupe de variables -->
<!-- * Avec la contrainte d’égaliser l’information apportée par chaque groupe (pondération des variables) dans l’analyse globale -->
<!-- * L’analyse simultanée des axes factoriels principaux issus de l’ensemble des variables et des groupes ainsi que le positionnement des variables initiales informent sur l’intensité de la relation entre les groupes de variables et la/les structure(s) commune(s) à l’ensemble -->

<!-- ## Mise en oeuvre de l’analyse factorielle multiple -->

<!-- L’algorithme AFMULT^[Escofier, B. and Pagès, J. (1994) *Multiple Factor Analysis (AFMULT package)*, Computational Statistics and Data Analysis, **18**, 121-140.] se décompose en 3 étapes : -->

<!-- 1. ACP et/ou ACM sur chaque groupe de variables -->
<!-- 2. Pondération des variables en fonction de la première valeur propre de l’ACP/ACM du groupe dont la/les variable(s) fait/font partie -->
<!--     + Chaque groupe apporte alors une unité d’information -->
<!-- 3. ACP sur l’ensemble des variables en tenant compte des pondérations (les axes factoriels principaux des ACP/ACM préliminaires sont ajoutés en variables illustratives) -->

<!-- ```{r} -->
<!-- Ronfle.MFA <- Ronfle[, c("Age", "IMC", "Alccol", "Sexe", "Ronfle", "Tabac", "AlcoolDisc", "AgeDisc", "IMCDisc")] -->

<!-- resMFA <- MFA(Ronfle.MFA, group = c(3,3,3), type = c("s", "n", "n"), ncp = 5, name.group = c("Quanti1", "Quali1", "Illustr"), num.group.sup = c(3), graph = FALSE) -->
<!-- resMFA -->
<!-- plot.MFA(resMFA, axes = c(1,2), choix = "group", lab.grpe = TRUE) -->
<!-- ``` -->

# ACP, AFCS, ACM

* 2 variables discrètes :

    $\rightarrow$ Analyse en factorielle des correspondances simples
(AFCS)

* Plus de 2 variables discrètes ou discrétisées :

    $\rightarrow$ Analyse des correspondances multiples (ACM)

* Plus de 2 variables continues ou pseudo-continues :

    $\rightarrow$ Analyse en composantes principales (ACP)
    
    $\rightarrow$ Analyse en factorielle des correspondances simples
(AFCS) si les variables ont les mêmes unités (ou sont standardisées par exemple)

    
# Pour aller plus loin
    
## La base de données `hobbies`

Base de données inclue dans le package `FactoMineR`. Utiliser `help(hobbies)` pour obtenir les détails des variables.

```{r}
data("hobbies") # importation du data set
dim(hobbies)
head(hobbies)
summary(hobbies)
str(hobbies)
```

### Exercices

1. Effectuez une analyse des correspondances multiples sur les données (en excluant la dernière variable, `nb.activitees`).
    a. Extraire les valeurs propres
    b. Visualiser les résultats pour les variables. Colorier le graphique en fonction des valeurs des cosinus carrés
    c. Visualiser les résultats pour les 10 individus avec les plus grands cosinus carrés (attention, le graphique avec tous les individus peut prendre beaucoup de temps)

```{r, warning = FALSE, message = FALSE}
#1. 
res.MCA <- MCA(hobbies[,-23], graph = FALSE)

#1a.
res.MCA$eig
#1b.
library(factoextra)
fviz_mca_var(res.MCA, 
             col.var="cos2", repel=TRUE,
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
#1c.
fviz_mca_ind(res.MCA, select.ind = list(cos2=10),
             col.var="cos2", repel=TRUE,
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```

2. Effectuer la même analyse sur les variables 1 à 18 (de `reading` à `TV`, sans les variables socio-démographiques).
    a. Visualiser seulement les 10 variables avec les plus grands cosinus carrés
    b. Comment peut-on interpréter ce graphique ?
    
```{r}
#2.
res.MCA2 <- MCA(hobbies[,-c(19:23)], graph = FALSE)

#2a.
fviz_mca_var(res.MCA2, 
             col.var="cos2", repel=TRUE, select.var = list(cos2=10),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

#2b.
#Groupement d'activités culturelles et ordinateur vs groupement sans ces activités
```

3. Effectuer la même analyse sur les variables 1 à 18 avec les variables 19 à 21 en variables illustratives.
    a. Visualiser seulement les 10 variables avec les plus grands cosinus carrés.
    b. Comment peut-on interpréter ce graphique ?
    
```{r}
#3.
res.MCA3 <- MCA(hobbies[,-c(22,23)], quali.sup = 19:21, graph = FALSE)

#3a
fviz_mca_var(res.MCA3, 
             col.var="cos2", repel=TRUE, select.var = list(cos2=10),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

#3b. Groupement d'activités culturelles plus associé aux personnes célibataires et entre 26 et 35 ans, 
#groupement non culturel plus associé aux personnes mariées, veuves et entre 66 et 85 ans
```